# Information_Retrieval_Project_Spring_2018

Project : Information_Retrieval_Project_Spring_2018
Team Members :
1. Nipesh Roy
2. Srashti Badjatya
3. Surupa Chatterjee

Setup Instructions : <br/>
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  <br/>
REQUIREMENTS:

1. Python version 3.6.1

2. Intsall requests using the following command:
   pip install requests

3. Install BeautifulSoup using the following command:
   pip install BeautifuSoup4

4. Install PyEnchant using the following command:
   pip install PyEnchant

5. Lucene version 4.2.7 

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  <br/>
HOW TO SETUP:

Using the terminal:

NOTE: If using Terminal, python environment must be setup for the terminal commands to work. 

- Navigate to the directory in which the source code with the py extension exists and run the following command in the terminal window :
	
	python3 <_filename_>.py

Example: 
	
	python3 BM25_Model.py 
	python3 generate_cleaned_corpus.py 


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  <br/>
Phase 1, Task 1:  Indexing and Retrieval

Java Libraries used:

1. java.io.* 
2. java.util.* 
3. org.apache.lucene.*

Python Libraries used:

1. import math
2. import operator
3. import requests
4. from bs4 import BeautifulSoup
5. import os
6. import re
7. import string
8. import operator

Task1 Files:
-- In Lucene (folder) :  <br/>

	1. Lucene_Model_4.2.7.java : Contains source code for Lucene indexing and retrieval Model.

	NOTE : - Run as Java application in Eclipse and follow the steps are shown on the console.
	             - Remember to add lucene's required jars in class path before running the source code.
	             - Remember to delete all the files generated by lucene for every fresh run.	

	2. Lucene_Ranking.txt : Contains the top 100 documents obtained for the given 64 queries respectively using lucene-4.2.7.
	                                      
	NOTE:  Formats :  Query : q 
	       query_id Q0  doc_id rank system_name (lucene_4.2.7, BM25, QLM_35, TF_iDF)

-- In BM25 (folder) :  <br/>

	1. BM25_Model.py : Contains source code for BM25 retrieval model.

	2. BM25_Ranking.txt : Contains the top 100 documents obtained for the given 64 queries 	respectively using BM25 retrieval model.

-- In Query_Likelihood_Model (folder) :  <br/>

	1. Query_Likelihood_Model_0.35.py : Contains source code for BM25 retrieval model.

	2. Query_Likelihood_Model_0.35_Ranking.txt : Contains the top 100 documents obtained for the given 64 queries respectively using BM25 retrieval model.

-- In TF.iDF (folder) :  <br/>

	1. TF_iDF.py : Contains source code for tf.idf retrieval model.

	2. TF_iDF_Ranking.txt : Contains the top 100 documents obtained for the given 64 queries respectively using tf.idf retrieval model.

-- In Indexer (folder) :  <br/>

	1. generate_clean_corpus.py : Contains source code for creating uni-gram inverted indexes.

-- Results from Indexer and Query Processing :  <br/>

	1. processed_queries.txt : Contains the given 64 queries in following format "query_id:q" 

	2. cleaned_files (folder) : Contains the corpus of CACM documents, which are cleaned as per the requirements (case-folding, punctuation handing, stopping and stemming).

	3. unigram_df.txt, unigram_terms.txt, unigram_tf.txt, inverted_index_unigram.txt : Contains information required for calculating ranking scores (used by BM25_Model.py, Query_Likelihood_Model_0.35.py, TF_iDF.py).

NOTE :  <br/>
		* Run generate_clean_corpus.py, processQueries.py  before running any of the retrieval models : BM25, Lucene, Smoothed Query Likelihood Model (lambda = 0.35) and tf.idf in-order to create inverted index and other related files from given corpus for retrieval.  <br/>
		* Delete the all intermediate and result files before every run.  <br/>

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  <br/>
Phase 1 : Task 2 : Query Enrichment(Query Expansion using Psuedo Relevance Feedback)<br/>

Python Libraries used:

import os.path<br/>
import traceback<br/>
import math<br/>

Task2 Files:

In PsuedoRelevanceFeedback (folder) :<br/>

	1. bm25_psudeo_relevance_feedback.py : Contains the code for query expansion using psuedo relevance feedback
	
	2. BM25_Model.py :  Contains source code for BM25 retrieval model.It is been used to generate the scores through BM25 
			    retrieval model after query is expanded
In Results (folder) :,br/>

	1. enriched_queries.txt : Contains expanded queries using pseudo relevance feedback
	
	2. BM25_Ranking_After_Query_Enriched.txt : Contains the top 100 documents obtained for the enriched 64 queries 
						   respectively using BM25 retrieval model.


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ <br/>

Phase 1 : Task 3 : Stopping and Stemming<br/>

Python Libraries used:

import os<br/>
import re<br/>
import operator<br/>
from bs4 import BeautifulSoup<br/>
import string<br/>

Task3 Files:

In Stemming (folder) :<br/>

	1. In BM25 folder:
		1. BM25_Model.py - Contains code for generating rankings for the stemmed queries using BM25 model.
		2. BM25_Ranking_with_stemming.txt -  Contains the top 100 documents obtained for the 7 stemmed queries 
						     respectively using BM25 retrieval model
	
	2. In Query_Likelihood_Model_0.35 folder:
		1. Query_Likelihood_Model_0.35.py - Contains code for generating rankings for the stemmed queries using
						    query likelihood model.
		2. Query_Likelihood_Model_0.35_Ranking_with_stemming.txt - Contains the top 100 documents obtained for the stemmed 
									   queries respectively using query likelihood retrieval model.
	
	3. In TF-iDF folder:
		1. TF-iDF.py - Contains code for generating rankings for the stemmed queries using tf-idf model.
		2. TF-iDF_Ranking_with_stemming.txt -  Contains the top 100 documents obtained for the 7 stemmed queries 
						       respectively using TF-iDF retrieval model

	4. Stemming_Parse_and_Index.py : Contains the code for parsing and generating index for cacm_stem.txt
	
	5. processQueriesStemming.py : Contains the code to give 7 queries in following format "query_id:q"  

In Stopping (folder) :<br/>

	1. In BM25 folder:
		1. BM25_Model.py - Contains code for generating rankings for the stopped queries using BM25 model.
		2. BM25_Ranking_with_stopping.txt -  Contains the top 100 documents obtained for the 64 stopped queries 
						     respectively using BM25 retrieval model
	
	2. In Query_Likelihood_Model_0.35 folder:
		1. Query_Likelihood_Model_0.35.py - Contains code for generating rankings for the stopped queries using
						    query likelihood model.
		2. Query_Likelihood_Model_0.35_Ranking_with_stopping.txt - Contains the top 100 documents obtained for the 64 stopped
									   queries respectively using query likelihood retrieval model.
	
	3. In TF-iDF folder:
		1. TF-iDF.py - Contains code for generating rankings for the stopped queries using tf-idf model.
		2. TF-iDF_Ranking_with_stopping.txt -  Contains the top 100 documents obtained for the 64 stopped queries 
						       respectively using TF-iDF retrieval model

	4. Stopping_Parse_and_Index.py : Contains the code for parsing and generating index after stopping from common_words.txt
	
	5. processQueriesStopping.py : Contains the code to give 64 queries in following format "query_id:q" after stopping

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ <br/>

Phase 2 : Snippet Generation  <br/>

NOTE : Run all the files related to Task 1.

-- In Phase2(Snippet_Generator) (folder) :  <br/>
	
	1. Snippet_Generator.py : Contains source code for generating snippets and highlighting query terms.

	2. Snippet_Generator_Results.html : Contains snippets and highlighted query terms for the top 100 documents obtained for the
					    given 64 queries respectively using BM25 retrieval model.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  <br/>

Phase 3 : Evaluation <br/>

NOTE : Run all the files related to Task 1,2,3 and Extra_credit.

-- In Phase3(Evaluation) (folder) :  <br/>
	
	1. evalutaion_results : Contains effectiveness for specific model as mentioned by the folder name.

	2. Evaluation.py : Calculate effective measures for all the models in Task 1,2,3 and extra_credit

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  <br/>

Extra Credit : 

Task 1 : Spelling Error Generator<br/>
Task 2 : Soft-query-matching to reduce impact of synthetic noise on effectiveness<br/>
Extra Credit (folder) : <br/>

	1. Spelling_error_generator.py : Contains code to generate noise in the query terms
	 	 			 spelling_error_queries.txt stored in Dataset folder which contains
	 				 Set of 64 queries that contain spelling errors
					 
	2. In BM25 folder: Contains ranking results and python code to generate ranking using BM25 retrieval model
	
	3. In Query_Likelihood_Model_0.35 folder: Contains ranking results and python code to generate ranking using query
						  likelihood retrieval model
	
	4. In TF-iDF folder: Contains ranking results and python code to generate ranking using TF-iDF retrieval model

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ <br/>

References/Citations :
1. https://www.crummy.com/software/BeautifulSoup/bs4/doc/
2. http://docs.python-requests.org/en/master/user/quickstart/
3. http://www.pythonforbeginners.com/requests/the-awesome-requests-module
4. http://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python
5. https://www.codecademy.com/learn/learn-python
6. https://docs.python.org/2/library/traceback.html
7. https://docs.python.org/2/library/os.html
8. https://docs.python.org/3/library/collections.html
9. https://docs.python.org/2/library/sets.html
10. https://en.wikipedia.org/wiki/Rocchio_algorithm
11. https://norvig.com/spell-correct.html
12. https://en.wikipedia.org/wiki/Levenshtein_distance
13. Stackoverflow for syntax, understanding certain concepts, like retrieval model formulae, application of python dictionaries, string
    library of python etc.

